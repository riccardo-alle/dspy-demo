{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"**DSPy**: Programming with Foundation Models\" - Demo\n",
    "## Part 1 - Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials(path_to_credentials: str) -> str:\n",
    "    with open(path_to_credentials, \"r\") as _f:\n",
    "        return _f.read()\n",
    "\n",
    "llm = dspy.OpenAI(\n",
    "    deployment_id=\"gpt-35-turbo\",\n",
    "    api_key=load_credentials(\"credentials.txt\"),\n",
    "    api_base=\"https://your_endpoint_name.openai.azure.com/\",\n",
    "    api_provider=\"azure\",\n",
    "    api_version=\"2023-03-15-preview\",\n",
    "    model_type=\"chat\"\n",
    ")\n",
    "\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dspy saves the \"history\" of your llm. So far, the history is empty\n",
    "# IMPORTANT NOTE: DSPy is caching LLMs calls\n",
    "# Querying an LLM with the same \"conditions\" (input, signature, etc.) will return the cached response\n",
    "llm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which LLMs are supported?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud-based and local models are both supported, however, at the moment:\n",
    "* among cloud-based,`OpenAI` is supported while Google is (`PaLM2`, `Gemini`...) not;\n",
    "* among the local models you find: `HFModel`, `vLLModel`, `OllamaModel`.\n",
    "\n",
    "Full information available at [DSPy | Documentation | Language Models](https://dspy-docs.vercel.app/docs/building-blocks/language_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures & Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A signature is a declarative specification of input/output behavior of a DSPy module.**\n",
    "* _What_ to do instead of _why_;\n",
    "* Writing signatures is far more modular, adaptive, and reproducible than writing prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"inline\" Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Wszystko OK, polecam!\"\n",
    "\n",
    "classify = dspy.Predict('review -> sentiment') # inline signature\n",
    "classify(review=review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature under the hood. For example, the `dspy.ChainOfThought` provide also the `rationale`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = dspy.ChainOfThought('review -> sentiment')\n",
    "result = classify(review=review)\n",
    "print(f\"Sentiment: {result.sentiment}\")\n",
    "print(f\"\\nRationale: {result.rationale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"class-based\" Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSentimentAnalysis(dspy.Signature):\n",
    "    \"\"\"review -> sentiment\"\"\"\n",
    "    \n",
    "    review = dspy.InputField()\n",
    "    sentiment = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = dspy.Predict(BasicSentimentAnalysis)\n",
    "classify(review=review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In clas-based Signatures, the docstring is part of the prompt\n",
    "# Input and Ouptut can be enriched by descriptions: `desc`\n",
    "class ReviewScoreEstimation(dspy.Signature):\n",
    "    \"\"\"review -> score\"\"\"\n",
    "    \n",
    "    review = dspy.InputField()\n",
    "    # `desc` will be used in the prompt by the model\n",
    "    score = dspy.OutputField(\n",
    "        desc=\"An integer between 1 and 5, where 1 represents extremely negative sentiment and 5 extremely positive\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = dspy.Predict(ReviewScoreEstimation)\n",
    "classify(review=review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the `llm` history!\n",
    "**Note: `dspy` is caching the llm's interactions**, so calling the llm with the same input and signatures will ouput the same response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use what we have learned so far to build our first `dspy` program!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Dummy) Business case: \n",
    "\n",
    "_\"Recent analys of the market suggest that consumers tend to mention specific product parameters when writing bad reviews\"_\n",
    "\n",
    "\n",
    "The BO of product reviews in your company wants to validate the hyphotheis above.\n",
    "We want to use a GenAI approach to prototype a solution and make a fast delivery of the results. The BO won't accept the solution as a black-box as she wants to know why the model made specific decisions. In general, we have a fixed budget allocated for GPT-3.5-turbo tests, so we want to query the model as least as possible.\n",
    "\n",
    "Given this, the specifications of our LLM program are:\n",
    "\n",
    "* Get the input review;\n",
    "* Predict the review score with a prediction rationale (business explanation);\n",
    "* If (and only if) the score is negative, look for parameters (costs optimization);\n",
    "* Produce a final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset source: https://github.com/allegro/klejbenchmark-allegroreviews\n",
    "reviews = [\n",
    "    {\"id\": \"69\", \"text\": \"Jakość dźwięku tragedia. Najgorsze ze słuchawek jakiekolwiek miałem okazje używać. Na 100% nie jest to produkt oryginalny Sony. Taka firma nigdy nie wypuściłaby  takiego złomu. Typowe podróby, 1000 razy gorsze od oryginalnego chińczyka. Nie ma sensu kupować nawet do rozmów telefonicznych, chyba ze kupimy je w firmowym salonie Sony, bo podejrzewam ze wówczas jakość ich będzie zadowalająca.\", \"ground_truth\": \"1\"},\n",
    "    {\"id\": \"122\", \"text\": \"Antena naprawdę duża  , obudowa plastikowa w moim przypadku przyszła dość porysowana , z minusów  bardzo tandetny uchwyt montażowy wykonany z aluminiowego kątownika który powoduje drżenie przy dużych wiatrach , zysk ponad 20dBi  a będzie jeszcze lepiej po skróceniu przewodu . Adapter na złącze SMA w zestawie , szkoda,że zrobiony na tak cienkim przewodzie. ogólnie bardzo dobra antena z małym ale.\", \"ground_truth\": \"4\"},\n",
    "    {\"id\": \"392\", \"text\": \"Klawiatura ma słabe ledy i są źle rozprowadzone, gdzie indziej jest jasno a gdzie indziej ciemno, klawisze się zcierają. Myszka jak dla mnie nie ma wad, dobrze mi się nią obsługuje, podkładka się szybko ztarła, a od początku wypakowania była zgięta, rogi podkładki są podwinięte. Słuchawki maja przeciętny dźwięk, materiał zastosowany po dłuższym użytkowaniu robi się twardy i nieprzyjemny, co jakiś czas muszę je zdejmować, bo bolą mnie od nich uszy i głowa, mikrofon nie trzeszczy\", \"ground_truth\": \"2\"},\n",
    "    {\"id\": \"396\", \"text\": \"Produkt niezgodny ze specyfikacją. Według opisu ma gniazdo jack  3,5 mm do podłączenia zestawu słuchawkowego. W praktyce otrzymałem jakąś zubożoną wersję produktu bez tego gniazda. A jedną z głównych funkcji różnych akcesoriów jabra jest ich wzajemna kompatibilność. Dodatkowo mam wrażenie, że produkt nie jest nowy, brakuje mu folii zabezpieczającej (patrz foto - na pierwszym zdjęciu jak powinna wyglądać folia zabezpieczająca, na drugim zdjęciu - na moim urządzeniu tej folii było brak)\", \"ground_truth\": \"1\"},\n",
    "    {\"id\": \"276\", \"text\": \"STRASZNIE GRUBE RAMKI! Bez piłowania się nie obyło, co i tak nie wystarczyło, bo o ile można spiłować boki na szerokość/długość, to raczej trudno spiłować boki na grubość/głębokość, chyba, że ktoś posiada specjalistyczne narzędzia typu dremel... Co do rozdzielczości to nie mam zastrzeżeń. Jakość w miarę podobna jak w oryginale. Tylko sprawne oko może zauważyć różnicę. Podsumowując, jeśli chcesz zaoszczędzić i kupić ten wyświetlacz za 54zł to potem się nie dziw, że ci on nie pasuje do obudowy i będziesz musiał piłować pilnikami i papierem ściernym. \", \"ground_truth\": \"3\"},\n",
    "    {\"id\": \"355\", \"text\": \"Rewelacyjne kolumny, zdecydowanie lepsza jakość dzwieku od wersji bez SE która również posiadam. Taga to najlepsze kolumny w tej cenie słuchałem Jamo i Yamahy ale Taga zdecydowanie mi bardziej podchodzi zarówno optycznie jak i barwa dzwięku. Wersja SE ma zdecydowanie bardziej klarowny dzwięk i świetne panele z tyłu do podłączanie kabli POLECAM!!!! Lakier fortepianowy to ogromna zaleta tych kolumn wygląda dobrze nawet po 2 latach nic sie z tym nie dziej złego :)\", \"ground_truth\": \"5\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(reviews)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data type for data in DSPy is `Example`. You will use Examples to represent items in your training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "for _, row in df.iterrows():\n",
    "    train_examples.append(dspy.Example(review=row[\"text\"]).with_inputs(\"review\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use `ReviewScoreEstimation` signature for score estimation\n",
    "# But we need an additional signature for parameters extraction\n",
    "class ParametersExtractor(dspy.Signature):\n",
    "    \"\"\"review -> ['parameter_name_1', 'parameter_name_2', ...]\"\"\"\n",
    "    \n",
    "    review = dspy.InputField(\n",
    "        desc=\"A product review written by a consumer in a Polish e-commerce\"\n",
    "    )\n",
    "    parameters = dspy.OutputField(\n",
    "        desc=\"A list of extracted product parameter names\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewAnalyzer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.review_score_estimator = dspy.ChainOfThought(ReviewScoreEstimation)\n",
    "        self.parameters_extractor = dspy.Predict(ParametersExtractor)\n",
    "\n",
    "        self.score_threshold = 3\n",
    "\n",
    "    def forward(self, review):        \n",
    "        score_estimator_response = self.review_score_estimator(review=review.review)\n",
    "        score = int(score_estimator_response.score)\n",
    "        \n",
    "        if score > self.score_threshold:\n",
    "            return dspy.Prediction(\n",
    "                review=review.review,\n",
    "                score=score,\n",
    "                business_explanation=score_estimator_response.rationale\n",
    "            )\n",
    "            \n",
    "        mentioned_parameters = self.parameters_extractor(review=review.review).parameters\n",
    "            \n",
    "        return dspy.Prediction(\n",
    "            review=review.review,\n",
    "            score=score,\n",
    "            mentioned_parameters=mentioned_parameters,\n",
    "            business_explanation=score_estimator_response.rationale\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try our program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_analyzer = ReviewAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = review_analyzer.forward(input_example)\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we improve our results?\n",
    "\n",
    "Few ideas:\n",
    "\n",
    "* Improve the Signatures;\n",
    "* Use more advanced dspy blocks in the pipeline, e.g replace `Predict` with `ChainOfThought`;\n",
    "* Get more data, define a metric and let the pipeline **train** by itself! (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Optimization\n",
    "\n",
    "For this part we are going to use a tutorial provided by DSPy authors.\n",
    "\n",
    "The tutorial is about compiling a RAG pipeline using HotPotQA dataset and ColBERTv2 as a retriever module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag_schema](rag-schema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy provides a free-to-use retriever model for educational purposes\n",
    "# We are adding the retriever `rm` to our configuration. This will be used when calling `dspy.Retrieve`\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HotPotQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = trainset[0]\n",
    "print(f\"Question: {train_example.question}\")\n",
    "print(f\"Answer: {train_example.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_example = devset[10]\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Answer: {dev_example.answer}\")\n",
    "print(f\"Relevant Wikipedia Titles: {dev_example.gold_titles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve = dspy.Retrieve(k=3)\n",
    "topK_passages = retrieve(dev_example.question).passages\n",
    "\n",
    "print(f\"Top {retrieve.k} passages for question: {dev_example.question} \\n\", '-' * 30, '\\n')\n",
    "\n",
    "for idx, passage in enumerate(topK_passages):\n",
    "    print(f'{idx+1}]', passage, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSPy program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"question, context -> answer\"\"\"\n",
    "\n",
    "    context = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's now define some new key components of DSPy in order to perform automatic evaluation of our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a positive match when:\n",
    "# (generated answer and true answer match exactly) AND (the retrieved context does actually contain that answer)\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "evaluate_hotpot = Evaluate(\n",
    "    devset=devset,\n",
    "    metric=validate_context_and_answer,\n",
    "    num_threads=32,\n",
    "    display_progress=True,\n",
    "    display_table=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_hotpot(basic_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers (a.k.a Teleprompters)\n",
    "\n",
    "A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy. There are many built-in optimizers in DSPy, which apply vastly different strategies. A typical DSPy optimizer takes three things:\n",
    "\n",
    "* Your DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program.\n",
    "\n",
    "* Your metric. This is a function that evaluates the output of your program, and assigns it a score (higher is better).\n",
    "\n",
    "* A few training inputs. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer 1: BootstrapFewShot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses few-shot learning examples to bootstrap your program.\n",
    "* few-shot examples may come from a training set;\n",
    "* few-shot examples may be self-generated by the program itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "teleprompter_few_shot = BootstrapFewShot(\n",
    "    metric=validate_context_and_answer,       # it defines the metric to be optimized\n",
    "    max_bootstrapped_demos=2,                 # it defines the max number of bootstrapped \"demos\" of your program\n",
    "    max_labeled_demos=4,                      # it defines the max number of few-shot learning examples\n",
    "    max_rounds=2,                             # how many times to repeat the process before arresting it\n",
    "    teacher_settings={}                       # if set, the teacher LLM will be used to generate synthetic examples and the bootstrap logic\n",
    ")\n",
    "\n",
    "# Compile!\n",
    "compiled_few_shot_rag = teleprompter_few_shot.compile(RAG(), trainset=trainset)\n",
    "compiled_few_shot_rag.save('compiled_rag.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_hotpot(compiled_few_shot_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# How does our \"frozen\" RAG pipeline looks like?\n",
    "with open(\"compiled_rag.json\", \"r\") as json_file:\n",
    "    compiled_rag_json = json_file.read()\n",
    "    \n",
    "pprint(json.loads(compiled_rag_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teleprompter 2: SignatureOptimizer\n",
    "\n",
    "From the docs: Generates and refines new instructions for each step, and optimizes them with coordinate ascent.\n",
    "\n",
    "https://github.com/stanfordnlp/dspy/blob/main/dspy/teleprompt/signature_opt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import SignatureOptimizer\n",
    "\n",
    "teleprompter_signature = SignatureOptimizer(\n",
    "    metric=validate_context_and_answer,\n",
    "    breadth=10,                          # The number of new prompts to generate at each iteration. Default=10.\n",
    "    depth=3,                             # The number of times we should ask our prompt model to generate new prompts, with the history of the past prompts as input. Default=3\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile!\n",
    "kwargs = dict(num_threads=32, display_progress=True, display_table=0)\n",
    "compiled_rag_signature = teleprompter_signature.compile(RAG(), devset=devset, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_hotpot(compiled_rag_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our compiled programs \"in production\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Who did invent the computer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_few_shot_rag(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag_signature(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up an evaluation pipeline for the `ReviewAnalysis` DSPy program\n",
    "  * What should be the metric? Hint: we want correctly estimated score and correctly extracted parameters\n",
    "* Use an optimizer to improve the basic prompt;\n",
    "* Try out different modules from DSPy module zoo\n",
    "\n",
    "Enjoy programming -- not prompting -- LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
